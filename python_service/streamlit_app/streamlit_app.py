########################
### STREAMLIT_APP.PY ###
# streamlit_app.py starts up a streamlit app that acts as an interface to Flask API
# streamlit handles all the dashboard's UI and sends rest requests to Flask where actual
# processing is done.
########################

# importing necessary libraries
import time 
import numpy as np  
import pandas as pd 
import plotly.express as px  
import streamlit as st 
import sys 
import os 
import random
import requests 
from collections import deque
from yaml import KeyToken 

# importing local python modules 
sys.path.insert(1, './../../')

from python_service.kafka_twitter_app.kafka_twitter_app import KafkaTwitterApp
import python_service.streamlit_app.utils as utils
import logging

# creating functions 
def set_hashtag(hashtag):
    '''
    This function sends an HTTP request to flask api to change twitter's api
    filtered stream rules and start listening for tweets with the new hashtag. 
    Due to async nature of quering tweeter api, the function returns nothing. 

    Inputs:
        - hashtag : str, key word that represents a tweet hashtag to listen to
    '''
    logger.info(f'{hashtag}, in set hash funct')
    try:
        r = requests.post(url = "/".join([url, "set-hashtag"]), data = hashtag, timeout=0.01)
    except requests.exceptions.ReadTimeout: 
        logger.info("RequestFaield")
        pass
    return 

def stop_listening():
    '''
    This function sends an HTTP request to flask api to ask it to stop listening to filtered
    tweet stream. This is necessary since tweeter API only allows for 250,000 monthly tweets. 
    (this runs out very fast given hundreds of tweets generated every second)
    '''
    r = requests.post(url = "/".join([url,"stop-listening"]))  


### Config 
logger = logging.getLogger(__name__)
url = os.environ["API_URL"] # default "http://localhost:5000"


### UI 

# configure dashboard UI
st.set_page_config(
    page_title="Life Twitter Tag Dashboard",
    page_icon="ðŸ¤–",
    layout="wide",
)

# set dashboard title
st.title("Life Twitter Tag Dashboard")

# text input for hashtags 
hashtag = st.text_input("Hashtag to follow", "climate")
# boolean to show the graphs generated by stream
listening = False

# two buttons
but1, but2 = st.columns(2)
logger.info(f'{hashtag} outside button')
with but1:
    if st.button('Start Listening'):
        logger.info('button pressed')
        set_hashtag(hashtag)
        listening = True
with but2:       
    if st.button('Stop Listening'):
        stop_listening()
        listening = False

# display if started listening to steam
if listening:

    # start kafka consumer 
    kfapp = KafkaTwitterApp()
    kafka_consumer = kfapp.create_consumer()
    kafka_consumer.bootstrap_connected()

    # creating a single-element container
    placeholder = st.empty()

    # define all the running averages and queues 
    q_length = 100
    sentiment_q = deque()
    keyword_q = deque()

    end_time = time.time()

    counter = 1
    cumm_sentiment = 0
    cumm_freshness = 0
    cumm_tweets_ps = 0

    runn_avg_sentiment_old = 0
    runn_avg_freshness_old = 0
    runn_avg_tweets_ps_old = 0

    # iterating over the messages in the consumer 

    for message in kafka_consumer:

        start_time = time.time()
        contents = utils.decode_message(message.value)

        # keyword -- http request to api to get keywords from tweet 
        r_keyword = requests.get(url = "/".join([url, "get-keywords"]), data = contents['data']['text'].encode('utf-8'))
        r_keyword = r_keyword.json()
        top_bigram, keyword_list = r_keyword['top_bigram'], r_keyword['keyword_list'] 
        keyword_q.append(top_bigram)
        # sentiment  -- http request to api to get sentiment from tweet 
        r_sentiment = requests.get(url = "/".join([url, "get-sentiment"]), data = contents['data']['text'].encode('utf-8'))
        r_sentiment = r_sentiment.json()
        tweet_sentiment = r_sentiment['tweet_sentiment']

        sentiment_q.append(tweet_sentiment)
        sentiment_df = pd.DataFrame(dict(sentiment=sentiment_q,keyword=keyword_q, index=[i for i in range(len(sentiment_q))]))


        # freshness -- calcuate tweet freshness 
        created_at = contents['data']['created_at']
        freshness = utils.compute_freshness(created_at)
        delay = (start_time-end_time)
        sentiment = random.random()

        # update all running averages 
        cumm_sentiment += tweet_sentiment
        cumm_freshness += freshness
        cumm_tweets_ps += delay
        

        runn_avg_sentiment = round(cumm_sentiment / counter,2)
        runn_avg_freshness = round(cumm_freshness / counter,2)
        runn_avg_tweets_ps = round(cumm_tweets_ps / counter,2)

        delta_sentiment = round(runn_avg_sentiment_old - runn_avg_sentiment, 2)
        delta_freshness = round(runn_avg_freshness_old - runn_avg_freshness, 2)
        delta_tweets_ps = round(runn_avg_tweets_ps_old - runn_avg_tweets_ps, 2)

        # dispaly graphs 
        with placeholder.container():


            kpi1, kpi2, kpi3 = st.columns(3)
            kpi1.metric(
                label="Sentiment",
                value=round(runn_avg_sentiment,1),
                delta=delta_sentiment,
            )

            kpi2.metric(
                label="Tweets/s",
                value=round(runn_avg_tweets_ps,1),
                delta=delta_tweets_ps
            )

            kpi3.metric(
                label="Tweet freshness (s)",
                value=round(runn_avg_freshness, 1),
                delta=delta_freshness,
            )
            graph1, graph2 = st.columns(2)
            with graph1:
                st.markdown("### Sentiment Change")
                fig2 = px.scatter(data_frame=sentiment_df, x="index", y="sentiment", trendline="lowess", trendline_options=dict(frac=0.2))
                st.write(fig2)
            with graph2:
                st.markdown("### Keywords")
                fig3 = px.treemap(sentiment_df,  path=["keyword"], values='sentiment',
                        color='sentiment',
                        color_continuous_scale='RdYlGn',
                        color_continuous_midpoint=0)
                st.write(fig3)

            col1, col2 = st.columns(2)
            with col1: 
                st.markdown("Key Words")
                st.write(keyword_list)
            with col2:
                st.markdown("Tweet contents")
                st.write(contents['data']['text'])



        runn_avg_sentiment_old = runn_avg_sentiment
        runn_avg_freshness_old = runn_avg_freshness
        runn_avg_tweets_ps_old = runn_avg_tweets_ps
        counter += 1
        
        # start removing items from queue if its too long 
        if counter >= q_length:
            keyword_q.popleft()
            sentiment_q.popleft()

        end_time = time.time()


        
            
    